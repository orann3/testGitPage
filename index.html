<!DOCTYPE html>
<html>

<head>

  <meta charset="utf-8">
  <meta name="description"
    content="RoboMIND: Establishing a Benchmark for Multi-embodiment Intelligence Normative Data in Robot Manipulation.">
  <meta name="keywords" content="RoboMIND, Multi-embodiment Intelligence, Robot Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation</title>


  <script>
    window.dataLayer = window.dataLayer || [];
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <!-- <link href="./static/css/googlecss.css"
        rel="stylesheet"> -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="./static/js/jquerymin351.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://x-humanoid.com">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More 
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://x-humanoid.com/">
            X-Humanoid
          </a>
          <a class="navbar-item" href="https://github.com/x-humanoid-robomind">
            Github
          </a>
        </div>
      </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">RoboMIND: Benchmark on Multi-embodiment
              Intelligence Normative Data for Robot Manipulation </h1>
            <div class="is-size-5 ">
            <span class="author-block">Kun Wu<sup>1,∗</sup>,</span>
            <span class="author-block">Chengkai Hou<sup>2,3,∗</sup>,</span>
            <span class="author-block">Zhengping Che<sup>1,∗,†</sup>,</span>
            <br>
            <span class="author-block">Xiaozhu Ju<sup>1,∗</sup>1,∗,†</span>
            <span class="author-block">Zhuqin Yang<sup>1</sup>,</span>
            <span class="author-block">Meng Li<sup>1</sup>,</span>
            <span class="author-block">Yinuo Zhao<sup>1</sup>,</span>
            <span class="author-block">Zhiyuan Xu<sup>1</sup>,</span>
            <span class="author-block">Guang Yang<sup>1</sup>,</span>
            <span class="author-block">Zhen Zhao<sup>1</sup>,</span>
            <span class="author-block">Guangyu Li<sup>1</sup>,</span>
            <span class="author-block">Zhao Jin<sup>1</sup>,</span>
            <span class="author-block">Lecheng Wang<sup>1</sup>,</span>
            <span class="author-block">Jilei Mao<sup>1</sup>,</span>
            <span class="author-block">Xinhua Wang<sup>1</sup>,</span>
            <span class="author-block">Shichao Fan<sup>1</sup>,</span>
            <span class="author-block">Ning Liu<sup>1</sup>,</span>
            <span class="author-block">Pei Ren<sup>1</sup>,</span>
            <span class="author-block">Qiang Zhang<sup>1</sup>,</span>
            <span class="author-block">Yaoxu Lv<sup>2</sup>,</span>
            <span class="author-block">Mengzhen Liu<sup>2,3</sup>,</span>
            <span class="author-block">Jingyang He<sup>2,3</sup>,</span>
            <span class="author-block">Yulin Luo<sup>2,3</sup>,</span>
            <span class="author-block">Zeyu Gao<sup>3</sup>,</span>
            <span class="author-block">Chenxuan Li<sup>2</sup>,</span>
            <span class="author-block">Chenyang Gu<sup>2,3</sup>,</span>
            <span class="author-block">Yankai Fu<sup>2</sup>,</span>
            <span class="author-block">Di Wu<sup>2</sup>,</span>
            <span class="author-block">Xingyu Wang<sup>2</sup>,</span>
            <span class="author-block">Sixiang Chen<sup>2,3</sup>,</span>
            <span class="author-block">Zhenyu Wang<sup>2</sup>,</span>
            <span class="author-block">Pengju An<sup>2,3</sup>,</span>
            <span class="author-block">Siyuan Qian<sup>2,3</sup>,</span>
            <span class="author-block">
              Shanghang Zhang <sup>2,3
                <span class="icon">
                  <svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false"
                    data-prefix="fa" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg"
                    viewBox="0 0 512 512" data-fa-i2svg="">
                    <path fill="currentColor"
                      d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z">
                    </path>
                  </svg>
                
                </span>
              </sup>
             ,</span>
             <span class="author-block">
              Jian Tang<sup>1
                <span class="icon">
                  <svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false"
                    data-prefix="fa" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg"
                    viewBox="0 0 512 512" data-fa-i2svg="">
                    <path fill="currentColor"
                      d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z">
                    </path>
                  </svg>
                </span>
              </sup>
             ,</span>


            <div class="is-size-5 publication-authors id=institute">
              <span class="author-block"><sup>1</sup>Beijing Innovation Center of Humanoid Robotics,</span>
              <span class="author-block"><sup>2</sup>State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University,</span>
              <span class="author-block"><sup>3</sup>Beijing Academy of Artificial Intelligence</span>
              
            </div>

            
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>*</sup>Co-first Authors: Kun Wu, Chengkai Hou, Jiaming Liu, Zhengping Che,
                and Xiaozhu Ju,</span>
              <span class="author-block"><sup>†</sup>Corresponding Authors: Zhengping Che and Xiaozhu Ju</span>
              <span class="author-block">Project Leaders: Shanghang Zhang and Jian Tang</span>
              
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/x-humanoid-robomind"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://zitd5je6f7j.feishu.cn/share/base/form/shrcnOF6Ww4BuRWWtxljfs0aQqh"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div>

              
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video> -->
        <div class="yush-div-center">
          <img src="./static/images/allfigures.png" class="img-responsive">
        </div>

        <h2 class="subtitle has-text-centered">
          Overview of <span class="dnerf">RoboMIND</span>. We introduce RoboMIND (Multi-embodiment Intelligence Normative Data for Robot Manipulation), comprising 55k real-world demonstration trajectories across 279 diverse tasks involving 61 distinct object classes. To ensure consistency and reliability during policy learning, RoboMIND is gathered through human teleoperation and structured around a unified data collection standard. The four pie charts represent: (a) total trajectories categorized by embodiments, (b) trajectory lengths by embodiments, (c) total trajectories grouped by task categories, and (d) total trajectories based on object usage scenarios.
        </h2>
      </div>
    </div>
  </section>


  <!-- <section class="hero is-light is-small id=videos">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fail_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fail_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fail_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fail_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fail_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-mask">
            <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fail_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-coffee">
            <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fail_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fail_1.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->




  <section class="section" id="single-task-1">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Developing robust and general-purpose robotic manipulation policies is a key goal in the field of robotics. 
              To achieve effective generalization, it is essential to construct comprehensive datasets that encompass a large 
              number of demonstration trajectories and diverse tasks. Unlike vision or language data that can be collected 
              from the Internet, robotic datasets require detailed observations and manipulation actions, necessitating significant
               investment in hardware-software infrastructure and human labor. While existing works have focused on assembling 
               various individual robot datasets, there remains a lack of a unified data collection standard and insufficient 
               diversity in tasks, scenarios, and robot types.
            </p>
            <p>
              In this paper, we introduce RoboMIND (Multi-embodiment Intelligence Normative Data for Robot manipulation),
               featuring 55k real-world demonstration trajectories across 279 diverse tasks involving 61 different object 
               classes. RoboMIND is collected through human teleoperation and encompasses comprehensive robotic-related 
               information, including multi-view RGB-D images, proprioceptive robot state information, end effector details, 
               and linguistic task descriptions. To ensure dataset consistency and reliability during policy learning,
                RoboMIND is built on a unified data collection platform and standardized protocol, covering four distinct 
                robotic embodiments: the Franka Emika Panda, the UR-5e, the AgileX dual-arm robot, and the Tien Kung humanoid 
                robot with dual dexterous hands. In addition, we create a digital twin environment in the Isaac Sim simulator, 
                featuring the same tasks and assets as our real-world dataset. This simulation environment not only facilitates 
                the low-cost collection of additional training data but also enables efficient evaluation.
            </p>
            <p>
              We provide a thorough quantitative and qualitative analysis of RoboMIND across multiple dimensions, offering 
              detailed insights into the diversity of our datasets.
            </p>
            <p>
              In our experiments, we conduct extensive real-world testing with four state-of-the-art imitation learning 
              methods, demonstrating that training with RoboMIND data results in a high manipulation success rate and strong 
              generalization. In addition, investigations of failure reasons reveal promising directions for improvement.
            </p>

          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Data Overview. -->
      <!-- <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Data Overview</h2>
          <div class="content has-text-justified">
            <div class="yush-div-center">
              <img src="./static/images/overview.png" class="img-responsive">
            </div>

            <p> We introduce RoboMIND (Multi-embodiment Intelligence Normative Data for Robot Manipulation),
              comprising 55k real-world demonstration trajectories across 279 diverse tasks involving 61 distinct object
              classes. </p>

            <p> To ensure consistency and reliability during policy learning, RoboMIND is gathered through human
              teleoperation and structured around a unified data collection standard. The four pie charts represent:
              (a) total trajectories categorized by embodiments, (b) trajectory lengths by embodiments,
              (c) total trajectories grouped by task categories, and (d) total trajectories based on object usage
              scenarios. </p>

            <img src="./static/images/Comparison.png" class="img-responsive">
            <p> Comparison to existing real-world datasets for robot manipulation. All data is drawn from the original
              paper or from [41].
              We highlight advantages of RoboMIND in pink. </p>

          </div>
        </div>
      </div>
      <br /> -->
      <!-- / Data Overview. -->

  </section>

  <!-- <section class="section" id="demo"> -->
    <div class="container is-max-desktop">
      <!-- Results -->
      <!-- <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Results</h2> -->
      <!-- Results -->


      <div id="rdt-1">
        <!-- single-task imitation learning model. -->
        <!-- <h3 class="title is-4">Single-task imitation learning model</h3> -->
        <!-- <div class="content has-text-justified">
          <img src="./static/images/Success rates.png" class="img-responsive">
          <p> For single-task imitation learning model, such as ACT,
            which is specifically designed for single-task learning, we
            trained it from scratch using the RobotMind dataset and di
            rectly deployed it on the corresponding training tasks. Specifi
            cally, we employed ACT [93], DP [12], and BAKU [32] algo
            rithms, adhering to the default model settings recommended in
            their original publications. These experiments were performed
            across 45 tasks, distributed as follows: 15 tasks with the Franka
            robot, 15 tasks with AgileX, 10 tasks with Tien Kung, and 5
            tasks with UR-5e, which are already explained in detail in the
            Section V-A.


            Figure 9 presents the performance of ACT, BAKU and
            DP across 45 tasks using four types of robots, evaluated
            in terms of success rate. In Figure 9, we found that ACT
            achieved an average success rate of 55.3% across 15 tasks
            on AgileX, outperforming Franka (30.7%), UR-5e (38.0%),
            and Tien Kung (34.0%). The experiment results indicate that
            the ACT algorithm achieves at least one successful task
            completion on the majority of tasks, which demonstrates the
            effectiveness of the approach, as well as the accuracy of the
            visual perception and robotic joint information provided by the
            RoboMIND. </p>
          <p>
            <div class="columns is-centered"> -->

              <!-- Visual Effects. -->
              <!-- <div class="column">
                <div class="content">
                  <h2 class="title is-3">Visual Effects</h2>
                  <p>
                    Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
                    would be impossible without nerfies since it would require going through a wall.
                  </p>
                  <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                    <source src="./static/videos/dollyzoom-stacked.mp4" type="video/mp4">
                  </video>
                </div>
              </div> -->
              <!--/ Visual Effects. -->

              <!-- Matting. -->
              <!-- <div class="column">
                <h2 class="title is-3">Matting</h2>
                <div class="columns is-centered">
                  <div class="column content">
                    <p>
                      As a byproduct of our method, we can also solve the matting problem by ignoring
                      samples that fall outside of a bounding box during rendering.
                    </p>
                    <video id="matting-video" controls playsinline height="100%">
                      <source src="./static/videos/matting.mp4" type="video/mp4">
                    </video>
                  </div>

                </div>
              </div>
            </div> -->
            <!--/ Matting. -->
        </div>
      </div>
      <br>
      <div id="rdt-2">
        <!-- single-task imitation learning model. -->
        <!-- <h3 class="title is-4">Single-task imitation learning model</h3>
        <div class="content has-text-justified">
          <img src="./static/images/Success rates.png" class="img-responsive">
          <p> For single-task imitation learning model, such as ACT,
            which is specifically designed for single-task learning, we
            trained it from scratch using the RobotMind dataset and di
            rectly deployed it on the corresponding training tasks. Specifi
            cally, we employed ACT [93], DP [12], and BAKU [32] algo
            rithms, adhering to the default model settings recommended in
            their original publications. These experiments were performed
            across 45 tasks, distributed as follows: 15 tasks with the Franka
            robot, 15 tasks with AgileX, 10 tasks with Tien Kung, and 5
            tasks with UR-5e, which are already explained in detail in the
            Section V-A.


            Figure 9 presents the performance of ACT, BAKU and
            DP across 45 tasks using four types of robots, evaluated
            in terms of success rate. In Figure 9, we found that ACT
            achieved an average success rate of 55.3% across 15 tasks
            on AgileX, outperforming Franka (30.7%), UR-5e (38.0%),
            and Tien Kung (34.0%). The experiment results indicate that
            the ACT algorithm achieves at least one successful task
            completion on the majority of tasks, which demonstrates the
            effectiveness of the approach, as well as the accuracy of the
            visual perception and robotic joint information provided by the
            RoboMIND. </p>
          <p>
            <div class="columns is-centered"> -->

              <!-- Visual Effects. -->
              <!-- <div class="column">
                <div class="content">
                  <h2 class="title is-3">Visual Effects</h2>
                  <p>
                    Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
                    would be impossible without nerfies since it would require going through a wall.
                  </p>
                  <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                    <source src="./static/videos/dollyzoom-stacked.mp4" type="video/mp4">
                  </video>
                </div>
              </div> -->
              <!--/ Visual Effects. -->

              <!-- Matting. -->
              <!-- <div class="column">
                <h2 class="title is-3">Matting</h2>
                <div class="columns is-centered">
                  <div class="column content">
                    <p>
                      As a byproduct of our method, we can also solve the matting problem by ignoring
                      samples that fall outside of a bounding box during rendering.
                    </p>
                    <video id="matting-video" controls playsinline height="100%">
                      <source src="./static/videos/matting.mp4" type="video/mp4">
                    </video>
                  </div>

                </div>
              </div>
            </div> -->
            <!--/ Matting. -->
        </div>
      </div>
      <br>
      <div id="rdt-3">
        <!-- single-task imitation learning model. -->
        <!-- <h3 class="title is-4">Single-task imitation learning model</h3>
        <div class="content has-text-justified">
          <img src="./static/images/Success rates.png" class="img-responsive">
          <p> For single-task imitation learning model, such as ACT,
            which is specifically designed for single-task learning, we
            trained it from scratch using the RobotMind dataset and di
            rectly deployed it on the corresponding training tasks. Specifi
            cally, we employed ACT [93], DP [12], and BAKU [32] algo
            rithms, adhering to the default model settings recommended in
            their original publications. These experiments were performed
            across 45 tasks, distributed as follows: 15 tasks with the Franka
            robot, 15 tasks with AgileX, 10 tasks with Tien Kung, and 5
            tasks with UR-5e, which are already explained in detail in the
            Section V-A.


            Figure 9 presents the performance of ACT, BAKU and
            DP across 45 tasks using four types of robots, evaluated
            in terms of success rate. In Figure 9, we found that ACT
            achieved an average success rate of 55.3% across 15 tasks
            on AgileX, outperforming Franka (30.7%), UR-5e (38.0%),
            and Tien Kung (34.0%). The experiment results indicate that
            the ACT algorithm achieves at least one successful task
            completion on the majority of tasks, which demonstrates the
            effectiveness of the approach, as well as the accuracy of the
            visual perception and robotic joint information provided by the
            RoboMIND. </p>
          <p>
            <div class="columns is-centered"> -->

              <!-- Visual Effects. -->
              <!-- <div class="column">
                <div class="content">
                  <h2 class="title is-3">Visual Effects</h2>
                  <p>
                    Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
                    would be impossible without nerfies since it would require going through a wall.
                  </p>
                  <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
                    <source src="./static/videos/dollyzoom-stacked.mp4" type="video/mp4">
                  </video>
                </div>
              </div> -->
              <!--/ Visual Effects. -->

              <!-- Matting. -->
              <!-- <div class="column">
                <h2 class="title is-3">Matting</h2>
                <div class="columns is-centered">
                  <div class="column content">
                    <p>
                      As a byproduct of our method, we can also solve the matting problem by ignoring
                      samples that fall outside of a bounding box during rendering.
                    </p>
                    <video id="matting-video" controls playsinline height="100%">
                      <source src="./static/videos/matting.mp4" type="video/mp4">
                    </video>
                  </div>

                </div>
              </div>
            </div> -->
            <!--/ Matting. -->
        </div>
      </div>
  </section>








  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
    </div>
  </section> -->
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="./">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://x-humanoid-robomind.github.io/" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              The website template was borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
              and <a href="https://eureka-research.github.io/">Eureka</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>